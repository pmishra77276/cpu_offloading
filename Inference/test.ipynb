{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f7c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3.2 1B - 4-bit Quantization\n",
      "========================================\n",
      "\n",
      "1. Quantizing Llama 3.2 1B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.2-1B...\n",
      "Model loaded successfully!\n",
      "Model parameters: 1,235,814,400\n",
      "Starting 4-bit quantization...\n",
      "Quantizing layer: model.layers.0.self_attn.q_proj\n",
      "Quantizing layer: model.layers.0.self_attn.k_proj\n",
      "Quantizing layer: model.layers.0.self_attn.v_proj\n",
      "Quantizing layer: model.layers.0.self_attn.o_proj\n",
      "Quantizing layer: model.layers.0.mlp.gate_proj\n",
      "Quantizing layer: model.layers.0.mlp.up_proj\n",
      "Quantizing layer: model.layers.0.mlp.down_proj\n",
      "Quantizing layer: model.layers.1.self_attn.q_proj\n",
      "Quantizing layer: model.layers.1.self_attn.k_proj\n",
      "Quantizing layer: model.layers.1.self_attn.v_proj\n",
      "Quantizing layer: model.layers.1.self_attn.o_proj\n",
      "Quantizing layer: model.layers.1.mlp.gate_proj\n",
      "Quantizing layer: model.layers.1.mlp.up_proj\n",
      "Quantizing layer: model.layers.1.mlp.down_proj\n",
      "Quantizing layer: model.layers.2.self_attn.q_proj\n",
      "Quantizing layer: model.layers.2.self_attn.k_proj\n",
      "Quantizing layer: model.layers.2.self_attn.v_proj\n",
      "Quantizing layer: model.layers.2.self_attn.o_proj\n",
      "Quantizing layer: model.layers.2.mlp.gate_proj\n",
      "Quantizing layer: model.layers.2.mlp.up_proj\n",
      "Quantizing layer: model.layers.2.mlp.down_proj\n",
      "Quantizing layer: model.layers.3.self_attn.q_proj\n",
      "Quantizing layer: model.layers.3.self_attn.k_proj\n",
      "Quantizing layer: model.layers.3.self_attn.v_proj\n",
      "Quantizing layer: model.layers.3.self_attn.o_proj\n",
      "Quantizing layer: model.layers.3.mlp.gate_proj\n",
      "Quantizing layer: model.layers.3.mlp.up_proj\n",
      "Quantizing layer: model.layers.3.mlp.down_proj\n",
      "Quantizing layer: model.layers.4.self_attn.q_proj\n",
      "Quantizing layer: model.layers.4.self_attn.k_proj\n",
      "Quantizing layer: model.layers.4.self_attn.v_proj\n",
      "Quantizing layer: model.layers.4.self_attn.o_proj\n",
      "Quantizing layer: model.layers.4.mlp.gate_proj\n",
      "Quantizing layer: model.layers.4.mlp.up_proj\n",
      "Quantizing layer: model.layers.4.mlp.down_proj\n",
      "Quantizing layer: model.layers.5.self_attn.q_proj\n",
      "Quantizing layer: model.layers.5.self_attn.k_proj\n",
      "Quantizing layer: model.layers.5.self_attn.v_proj\n",
      "Quantizing layer: model.layers.5.self_attn.o_proj\n",
      "Quantizing layer: model.layers.5.mlp.gate_proj\n",
      "Quantizing layer: model.layers.5.mlp.up_proj\n",
      "Quantizing layer: model.layers.5.mlp.down_proj\n",
      "Quantizing layer: model.layers.6.self_attn.q_proj\n",
      "Quantizing layer: model.layers.6.self_attn.k_proj\n",
      "Quantizing layer: model.layers.6.self_attn.v_proj\n",
      "Quantizing layer: model.layers.6.self_attn.o_proj\n",
      "Quantizing layer: model.layers.6.mlp.gate_proj\n",
      "Quantizing layer: model.layers.6.mlp.up_proj\n",
      "Quantizing layer: model.layers.6.mlp.down_proj\n",
      "Quantizing layer: model.layers.7.self_attn.q_proj\n",
      "Quantizing layer: model.layers.7.self_attn.k_proj\n",
      "Quantizing layer: model.layers.7.self_attn.v_proj\n",
      "Quantizing layer: model.layers.7.self_attn.o_proj\n",
      "Quantizing layer: model.layers.7.mlp.gate_proj\n",
      "Quantizing layer: model.layers.7.mlp.up_proj\n",
      "Quantizing layer: model.layers.7.mlp.down_proj\n",
      "Quantizing layer: model.layers.8.self_attn.q_proj\n",
      "Quantizing layer: model.layers.8.self_attn.k_proj\n",
      "Quantizing layer: model.layers.8.self_attn.v_proj\n",
      "Quantizing layer: model.layers.8.self_attn.o_proj\n",
      "Quantizing layer: model.layers.8.mlp.gate_proj\n",
      "Quantizing layer: model.layers.8.mlp.up_proj\n",
      "Quantizing layer: model.layers.8.mlp.down_proj\n",
      "Quantizing layer: model.layers.9.self_attn.q_proj\n",
      "Quantizing layer: model.layers.9.self_attn.k_proj\n",
      "Quantizing layer: model.layers.9.self_attn.v_proj\n",
      "Quantizing layer: model.layers.9.self_attn.o_proj\n",
      "Quantizing layer: model.layers.9.mlp.gate_proj\n",
      "Quantizing layer: model.layers.9.mlp.up_proj\n",
      "Quantizing layer: model.layers.9.mlp.down_proj\n",
      "Quantizing layer: model.layers.10.self_attn.q_proj\n",
      "Quantizing layer: model.layers.10.self_attn.k_proj\n",
      "Quantizing layer: model.layers.10.self_attn.v_proj\n",
      "Quantizing layer: model.layers.10.self_attn.o_proj\n",
      "Quantizing layer: model.layers.10.mlp.gate_proj\n",
      "Quantizing layer: model.layers.10.mlp.up_proj\n",
      "Quantizing layer: model.layers.10.mlp.down_proj\n",
      "Quantizing layer: model.layers.11.self_attn.q_proj\n",
      "Quantizing layer: model.layers.11.self_attn.k_proj\n",
      "Quantizing layer: model.layers.11.self_attn.v_proj\n",
      "Quantizing layer: model.layers.11.self_attn.o_proj\n",
      "Quantizing layer: model.layers.11.mlp.gate_proj\n",
      "Quantizing layer: model.layers.11.mlp.up_proj\n",
      "Quantizing layer: model.layers.11.mlp.down_proj\n",
      "Quantizing layer: model.layers.12.self_attn.q_proj\n",
      "Quantizing layer: model.layers.12.self_attn.k_proj\n",
      "Quantizing layer: model.layers.12.self_attn.v_proj\n",
      "Quantizing layer: model.layers.12.self_attn.o_proj\n",
      "Quantizing layer: model.layers.12.mlp.gate_proj\n",
      "Quantizing layer: model.layers.12.mlp.up_proj\n",
      "Quantizing layer: model.layers.12.mlp.down_proj\n",
      "Quantizing layer: model.layers.13.self_attn.q_proj\n",
      "Quantizing layer: model.layers.13.self_attn.k_proj\n",
      "Quantizing layer: model.layers.13.self_attn.v_proj\n",
      "Quantizing layer: model.layers.13.self_attn.o_proj\n",
      "Quantizing layer: model.layers.13.mlp.gate_proj\n",
      "Quantizing layer: model.layers.13.mlp.up_proj\n",
      "Quantizing layer: model.layers.13.mlp.down_proj\n",
      "Quantizing layer: model.layers.14.self_attn.q_proj\n",
      "Quantizing layer: model.layers.14.self_attn.k_proj\n",
      "Quantizing layer: model.layers.14.self_attn.v_proj\n",
      "Quantizing layer: model.layers.14.self_attn.o_proj\n",
      "Quantizing layer: model.layers.14.mlp.gate_proj\n",
      "Quantizing layer: model.layers.14.mlp.up_proj\n",
      "Quantizing layer: model.layers.14.mlp.down_proj\n",
      "Quantizing layer: model.layers.15.self_attn.q_proj\n",
      "Quantizing layer: model.layers.15.self_attn.k_proj\n",
      "Quantizing layer: model.layers.15.self_attn.v_proj\n",
      "Quantizing layer: model.layers.15.self_attn.o_proj\n",
      "Quantizing layer: model.layers.15.mlp.gate_proj\n",
      "Quantizing layer: model.layers.15.mlp.up_proj\n",
      "Quantizing layer: model.layers.15.mlp.down_proj\n",
      "Quantizing layer: lm_head\n",
      "\n",
      "Quantization completed!\n",
      "- Total Linear layers found: 113\n",
      "- Layers quantized: 113\n",
      "- Original model size: 4714.26 MB\n",
      "- Quantized model size: 1002.26 MB\n",
      "- Size reduction: 4.70x\n",
      "- Space saved: 3712.00 MB\n",
      "\n",
      "2. Testing text generation...\n",
      "Loading meta-llama/Llama-3.2-1B...\n",
      "Model loaded successfully!\n",
      "Model parameters: 1,235,814,400\n",
      "Starting 4-bit quantization...\n",
      "Quantizing layer: model.layers.0.self_attn.q_proj\n",
      "Quantizing layer: model.layers.0.self_attn.k_proj\n",
      "Quantizing layer: model.layers.0.self_attn.v_proj\n",
      "Quantizing layer: model.layers.0.self_attn.o_proj\n",
      "Quantizing layer: model.layers.0.mlp.gate_proj\n",
      "Quantizing layer: model.layers.0.mlp.up_proj\n",
      "Quantizing layer: model.layers.0.mlp.down_proj\n",
      "Quantizing layer: model.layers.1.self_attn.q_proj\n",
      "Quantizing layer: model.layers.1.self_attn.k_proj\n",
      "Quantizing layer: model.layers.1.self_attn.v_proj\n",
      "Quantizing layer: model.layers.1.self_attn.o_proj\n",
      "Quantizing layer: model.layers.1.mlp.gate_proj\n",
      "Quantizing layer: model.layers.1.mlp.up_proj\n",
      "Quantizing layer: model.layers.1.mlp.down_proj\n",
      "Quantizing layer: model.layers.2.self_attn.q_proj\n",
      "Quantizing layer: model.layers.2.self_attn.k_proj\n",
      "Quantizing layer: model.layers.2.self_attn.v_proj\n",
      "Quantizing layer: model.layers.2.self_attn.o_proj\n",
      "Quantizing layer: model.layers.2.mlp.gate_proj\n",
      "Quantizing layer: model.layers.2.mlp.up_proj\n",
      "Quantizing layer: model.layers.2.mlp.down_proj\n",
      "Quantizing layer: model.layers.3.self_attn.q_proj\n",
      "Quantizing layer: model.layers.3.self_attn.k_proj\n",
      "Quantizing layer: model.layers.3.self_attn.v_proj\n",
      "Quantizing layer: model.layers.3.self_attn.o_proj\n",
      "Quantizing layer: model.layers.3.mlp.gate_proj\n",
      "Quantizing layer: model.layers.3.mlp.up_proj\n",
      "Quantizing layer: model.layers.3.mlp.down_proj\n",
      "Quantizing layer: model.layers.4.self_attn.q_proj\n",
      "Quantizing layer: model.layers.4.self_attn.k_proj\n",
      "Quantizing layer: model.layers.4.self_attn.v_proj\n",
      "Quantizing layer: model.layers.4.self_attn.o_proj\n",
      "Quantizing layer: model.layers.4.mlp.gate_proj\n",
      "Quantizing layer: model.layers.4.mlp.up_proj\n",
      "Quantizing layer: model.layers.4.mlp.down_proj\n",
      "Quantizing layer: model.layers.5.self_attn.q_proj\n",
      "Quantizing layer: model.layers.5.self_attn.k_proj\n",
      "Quantizing layer: model.layers.5.self_attn.v_proj\n",
      "Quantizing layer: model.layers.5.self_attn.o_proj\n",
      "Quantizing layer: model.layers.5.mlp.gate_proj\n",
      "Quantizing layer: model.layers.5.mlp.up_proj\n",
      "Quantizing layer: model.layers.5.mlp.down_proj\n",
      "Quantizing layer: model.layers.6.self_attn.q_proj\n",
      "Quantizing layer: model.layers.6.self_attn.k_proj\n",
      "Quantizing layer: model.layers.6.self_attn.v_proj\n",
      "Quantizing layer: model.layers.6.self_attn.o_proj\n",
      "Quantizing layer: model.layers.6.mlp.gate_proj\n",
      "Quantizing layer: model.layers.6.mlp.up_proj\n",
      "Quantizing layer: model.layers.6.mlp.down_proj\n",
      "Quantizing layer: model.layers.7.self_attn.q_proj\n",
      "Quantizing layer: model.layers.7.self_attn.k_proj\n",
      "Quantizing layer: model.layers.7.self_attn.v_proj\n",
      "Quantizing layer: model.layers.7.self_attn.o_proj\n",
      "Quantizing layer: model.layers.7.mlp.gate_proj\n",
      "Quantizing layer: model.layers.7.mlp.up_proj\n",
      "Quantizing layer: model.layers.7.mlp.down_proj\n",
      "Quantizing layer: model.layers.8.self_attn.q_proj\n",
      "Quantizing layer: model.layers.8.self_attn.k_proj\n",
      "Quantizing layer: model.layers.8.self_attn.v_proj\n",
      "Quantizing layer: model.layers.8.self_attn.o_proj\n",
      "Quantizing layer: model.layers.8.mlp.gate_proj\n",
      "Quantizing layer: model.layers.8.mlp.up_proj\n",
      "Quantizing layer: model.layers.8.mlp.down_proj\n",
      "Quantizing layer: model.layers.9.self_attn.q_proj\n",
      "Quantizing layer: model.layers.9.self_attn.k_proj\n",
      "Quantizing layer: model.layers.9.self_attn.v_proj\n",
      "Quantizing layer: model.layers.9.self_attn.o_proj\n",
      "Quantizing layer: model.layers.9.mlp.gate_proj\n",
      "Quantizing layer: model.layers.9.mlp.up_proj\n",
      "Quantizing layer: model.layers.9.mlp.down_proj\n",
      "Quantizing layer: model.layers.10.self_attn.q_proj\n",
      "Quantizing layer: model.layers.10.self_attn.k_proj\n",
      "Quantizing layer: model.layers.10.self_attn.v_proj\n",
      "Quantizing layer: model.layers.10.self_attn.o_proj\n",
      "Quantizing layer: model.layers.10.mlp.gate_proj\n",
      "Quantizing layer: model.layers.10.mlp.up_proj\n",
      "Quantizing layer: model.layers.10.mlp.down_proj\n",
      "Quantizing layer: model.layers.11.self_attn.q_proj\n",
      "Quantizing layer: model.layers.11.self_attn.k_proj\n",
      "Quantizing layer: model.layers.11.self_attn.v_proj\n",
      "Quantizing layer: model.layers.11.self_attn.o_proj\n",
      "Quantizing layer: model.layers.11.mlp.gate_proj\n",
      "Quantizing layer: model.layers.11.mlp.up_proj\n",
      "Quantizing layer: model.layers.11.mlp.down_proj\n",
      "Quantizing layer: model.layers.12.self_attn.q_proj\n",
      "Quantizing layer: model.layers.12.self_attn.k_proj\n",
      "Quantizing layer: model.layers.12.self_attn.v_proj\n",
      "Quantizing layer: model.layers.12.self_attn.o_proj\n",
      "Quantizing layer: model.layers.12.mlp.gate_proj\n",
      "Quantizing layer: model.layers.12.mlp.up_proj\n",
      "Quantizing layer: model.layers.12.mlp.down_proj\n",
      "Quantizing layer: model.layers.13.self_attn.q_proj\n",
      "Quantizing layer: model.layers.13.self_attn.k_proj\n",
      "Quantizing layer: model.layers.13.self_attn.v_proj\n",
      "Quantizing layer: model.layers.13.self_attn.o_proj\n",
      "Quantizing layer: model.layers.13.mlp.gate_proj\n",
      "Quantizing layer: model.layers.13.mlp.up_proj\n",
      "Quantizing layer: model.layers.13.mlp.down_proj\n",
      "Quantizing layer: model.layers.14.self_attn.q_proj\n",
      "Quantizing layer: model.layers.14.self_attn.k_proj\n",
      "Quantizing layer: model.layers.14.self_attn.v_proj\n",
      "Quantizing layer: model.layers.14.self_attn.o_proj\n",
      "Quantizing layer: model.layers.14.mlp.gate_proj\n",
      "Quantizing layer: model.layers.14.mlp.up_proj\n",
      "Quantizing layer: model.layers.14.mlp.down_proj\n",
      "Quantizing layer: model.layers.15.self_attn.q_proj\n",
      "Quantizing layer: model.layers.15.self_attn.k_proj\n",
      "Quantizing layer: model.layers.15.self_attn.v_proj\n",
      "Quantizing layer: model.layers.15.self_attn.o_proj\n",
      "Quantizing layer: model.layers.15.mlp.gate_proj\n",
      "Quantizing layer: model.layers.15.mlp.up_proj\n",
      "Quantizing layer: model.layers.15.mlp.down_proj\n",
      "Quantizing layer: lm_head\n",
      "\n",
      "Quantization completed!\n",
      "- Total Linear layers found: 113\n",
      "- Layers quantized: 113\n",
      "- Original model size: 4714.26 MB\n",
      "- Quantized model size: 1002.26 MB\n",
      "- Size reduction: 4.70x\n",
      "- Space saved: 3712.00 MB\n",
      "\n",
      "Testing with prompt: 'The future of artificial intelligence is'\n",
      "\n",
      "Generating text with quantized model...\n",
      "\n",
      "Generated text:\n",
      "'The future of artificial intelligence isInputStreamagneagneagneagneagneagnepliers craftesignesignesignesignesignesignesignenkoenkogiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengiengien'\n",
      "\n",
      "3. Comparing original vs quantized outputs...\n",
      "Loading original model...\n",
      "Quantizing model...\n",
      "Starting 4-bit quantization...\n",
      "Quantizing layer: model.layers.0.self_attn.q_proj\n",
      "Quantizing layer: model.layers.0.self_attn.k_proj\n",
      "Quantizing layer: model.layers.0.self_attn.v_proj\n",
      "Quantizing layer: model.layers.0.self_attn.o_proj\n",
      "Quantizing layer: model.layers.0.mlp.gate_proj\n",
      "Quantizing layer: model.layers.0.mlp.up_proj\n",
      "Quantizing layer: model.layers.0.mlp.down_proj\n",
      "Quantizing layer: model.layers.1.self_attn.q_proj\n",
      "Quantizing layer: model.layers.1.self_attn.k_proj\n",
      "Quantizing layer: model.layers.1.self_attn.v_proj\n",
      "Quantizing layer: model.layers.1.self_attn.o_proj\n",
      "Quantizing layer: model.layers.1.mlp.gate_proj\n",
      "Quantizing layer: model.layers.1.mlp.up_proj\n",
      "Quantizing layer: model.layers.1.mlp.down_proj\n",
      "Quantizing layer: model.layers.2.self_attn.q_proj\n",
      "Quantizing layer: model.layers.2.self_attn.k_proj\n",
      "Quantizing layer: model.layers.2.self_attn.v_proj\n",
      "Quantizing layer: model.layers.2.self_attn.o_proj\n",
      "Quantizing layer: model.layers.2.mlp.gate_proj\n",
      "Quantizing layer: model.layers.2.mlp.up_proj\n",
      "Quantizing layer: model.layers.2.mlp.down_proj\n",
      "Quantizing layer: model.layers.3.self_attn.q_proj\n",
      "Quantizing layer: model.layers.3.self_attn.k_proj\n",
      "Quantizing layer: model.layers.3.self_attn.v_proj\n",
      "Quantizing layer: model.layers.3.self_attn.o_proj\n",
      "Quantizing layer: model.layers.3.mlp.gate_proj\n",
      "Quantizing layer: model.layers.3.mlp.up_proj\n",
      "Quantizing layer: model.layers.3.mlp.down_proj\n",
      "Quantizing layer: model.layers.4.self_attn.q_proj\n",
      "Quantizing layer: model.layers.4.self_attn.k_proj\n",
      "Quantizing layer: model.layers.4.self_attn.v_proj\n",
      "Quantizing layer: model.layers.4.self_attn.o_proj\n",
      "Quantizing layer: model.layers.4.mlp.gate_proj\n",
      "Quantizing layer: model.layers.4.mlp.up_proj\n",
      "Quantizing layer: model.layers.4.mlp.down_proj\n",
      "Quantizing layer: model.layers.5.self_attn.q_proj\n",
      "Quantizing layer: model.layers.5.self_attn.k_proj\n",
      "Quantizing layer: model.layers.5.self_attn.v_proj\n",
      "Quantizing layer: model.layers.5.self_attn.o_proj\n",
      "Quantizing layer: model.layers.5.mlp.gate_proj\n",
      "Quantizing layer: model.layers.5.mlp.up_proj\n",
      "Quantizing layer: model.layers.5.mlp.down_proj\n",
      "Quantizing layer: model.layers.6.self_attn.q_proj\n",
      "Quantizing layer: model.layers.6.self_attn.k_proj\n",
      "Quantizing layer: model.layers.6.self_attn.v_proj\n",
      "Quantizing layer: model.layers.6.self_attn.o_proj\n",
      "Quantizing layer: model.layers.6.mlp.gate_proj\n",
      "Quantizing layer: model.layers.6.mlp.up_proj\n",
      "Quantizing layer: model.layers.6.mlp.down_proj\n",
      "Quantizing layer: model.layers.7.self_attn.q_proj\n",
      "Quantizing layer: model.layers.7.self_attn.k_proj\n",
      "Quantizing layer: model.layers.7.self_attn.v_proj\n",
      "Quantizing layer: model.layers.7.self_attn.o_proj\n",
      "Quantizing layer: model.layers.7.mlp.gate_proj\n",
      "Quantizing layer: model.layers.7.mlp.up_proj\n",
      "Quantizing layer: model.layers.7.mlp.down_proj\n",
      "Quantizing layer: model.layers.8.self_attn.q_proj\n",
      "Quantizing layer: model.layers.8.self_attn.k_proj\n",
      "Quantizing layer: model.layers.8.self_attn.v_proj\n",
      "Quantizing layer: model.layers.8.self_attn.o_proj\n",
      "Quantizing layer: model.layers.8.mlp.gate_proj\n",
      "Quantizing layer: model.layers.8.mlp.up_proj\n",
      "Quantizing layer: model.layers.8.mlp.down_proj\n",
      "Quantizing layer: model.layers.9.self_attn.q_proj\n",
      "Quantizing layer: model.layers.9.self_attn.k_proj\n",
      "Quantizing layer: model.layers.9.self_attn.v_proj\n",
      "Quantizing layer: model.layers.9.self_attn.o_proj\n",
      "Quantizing layer: model.layers.9.mlp.gate_proj\n",
      "Quantizing layer: model.layers.9.mlp.up_proj\n",
      "Quantizing layer: model.layers.9.mlp.down_proj\n",
      "Quantizing layer: model.layers.10.self_attn.q_proj\n",
      "Quantizing layer: model.layers.10.self_attn.k_proj\n",
      "Quantizing layer: model.layers.10.self_attn.v_proj\n",
      "Quantizing layer: model.layers.10.self_attn.o_proj\n",
      "Quantizing layer: model.layers.10.mlp.gate_proj\n",
      "Quantizing layer: model.layers.10.mlp.up_proj\n",
      "Quantizing layer: model.layers.10.mlp.down_proj\n",
      "Quantizing layer: model.layers.11.self_attn.q_proj\n",
      "Quantizing layer: model.layers.11.self_attn.k_proj\n",
      "Quantizing layer: model.layers.11.self_attn.v_proj\n",
      "Quantizing layer: model.layers.11.self_attn.o_proj\n",
      "Quantizing layer: model.layers.11.mlp.gate_proj\n",
      "Quantizing layer: model.layers.11.mlp.up_proj\n",
      "Quantizing layer: model.layers.11.mlp.down_proj\n",
      "Quantizing layer: model.layers.12.self_attn.q_proj\n",
      "Quantizing layer: model.layers.12.self_attn.k_proj\n",
      "Quantizing layer: model.layers.12.self_attn.v_proj\n",
      "Quantizing layer: model.layers.12.self_attn.o_proj\n",
      "Quantizing layer: model.layers.12.mlp.gate_proj\n",
      "Quantizing layer: model.layers.12.mlp.up_proj\n",
      "Quantizing layer: model.layers.12.mlp.down_proj\n",
      "Quantizing layer: model.layers.13.self_attn.q_proj\n",
      "Quantizing layer: model.layers.13.self_attn.k_proj\n",
      "Quantizing layer: model.layers.13.self_attn.v_proj\n",
      "Quantizing layer: model.layers.13.self_attn.o_proj\n",
      "Quantizing layer: model.layers.13.mlp.gate_proj\n",
      "Quantizing layer: model.layers.13.mlp.up_proj\n",
      "Quantizing layer: model.layers.13.mlp.down_proj\n",
      "Quantizing layer: model.layers.14.self_attn.q_proj\n",
      "Quantizing layer: model.layers.14.self_attn.k_proj\n",
      "Quantizing layer: model.layers.14.self_attn.v_proj\n",
      "Quantizing layer: model.layers.14.self_attn.o_proj\n",
      "Quantizing layer: model.layers.14.mlp.gate_proj\n",
      "Quantizing layer: model.layers.14.mlp.up_proj\n",
      "Quantizing layer: model.layers.14.mlp.down_proj\n",
      "Quantizing layer: model.layers.15.self_attn.q_proj\n",
      "Quantizing layer: model.layers.15.self_attn.k_proj\n",
      "Quantizing layer: model.layers.15.self_attn.v_proj\n",
      "Quantizing layer: model.layers.15.self_attn.o_proj\n",
      "Quantizing layer: model.layers.15.mlp.gate_proj\n",
      "Quantizing layer: model.layers.15.mlp.up_proj\n",
      "Quantizing layer: model.layers.15.mlp.down_proj\n",
      "Quantizing layer: lm_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantization completed!\n",
      "- Total Linear layers found: 113\n",
      "- Layers quantized: 113\n",
      "- Original model size: 4714.26 MB\n",
      "- Quantized model size: 1002.26 MB\n",
      "- Size reduction: 4.70x\n",
      "- Space saved: 3712.00 MB\n",
      "\n",
      "Comparing outputs for prompt: 'Artificial intelligence will'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import Dict, Any\n",
    "import copy\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"Custom 4-bit quantized linear layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, original_weight, original_bias=None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Quantize the weight matrix\n",
    "        quantized_weight, scale, zero_point = self.quantize_tensor_4bit(original_weight)\n",
    "        \n",
    "        # Store quantized parameters\n",
    "        self.register_buffer('quantized_weight', quantized_weight)\n",
    "        self.register_buffer('scale', scale)\n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        \n",
    "        if original_bias is not None:\n",
    "            self.register_buffer('bias', original_bias.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def quantize_tensor_4bit(self, tensor):\n",
    "        \"\"\"\n",
    "        Quantize tensor to 4-bit precision using asymmetric quantization\n",
    "        Returns: quantized_tensor, scale, zero_point\n",
    "        \"\"\"\n",
    "        # Flatten tensor for per-tensor quantization\n",
    "        flat_tensor = tensor.flatten()\n",
    "        \n",
    "        # Calculate min and max values\n",
    "        min_val = flat_tensor.min().item()\n",
    "        max_val = flat_tensor.max().item()\n",
    "        \n",
    "        # 4-bit can represent 16 values (0-15)\n",
    "        qmin, qmax = 0, 15\n",
    "        \n",
    "        # Calculate scale and zero point\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = max(qmin, min(qmax, round(zero_point)))\n",
    "        \n",
    "        # Quantize the tensor\n",
    "        quantized = torch.round(tensor / scale + zero_point)\n",
    "        quantized = torch.clamp(quantized, qmin, qmax)\n",
    "        \n",
    "        # Convert to uint8 for storage (we'll mask to 4-bit during computation)\n",
    "        quantized = quantized.to(torch.uint8)\n",
    "        \n",
    "        return quantized, torch.tensor(scale), torch.tensor(zero_point)\n",
    "    \n",
    "    def dequantize_tensor(self, quantized_tensor, scale, zero_point):\n",
    "        \"\"\"Dequantize tensor back to float\"\"\"\n",
    "        return scale * (quantized_tensor.float() - zero_point)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Dequantize weights for computation\n",
    "        dequantized_weight = self.dequantize_tensor(\n",
    "            self.quantized_weight, self.scale, self.zero_point\n",
    "        )\n",
    "        \n",
    "        # Perform linear operation\n",
    "        output = torch.nn.functional.linear(x, dequantized_weight, self.bias)\n",
    "        return output\n",
    "\n",
    "def calculate_model_size_reduction(original_model, quantized_model):\n",
    "    \"\"\"Calculate size reduction achieved by quantization\"\"\"\n",
    "    def get_model_size(model):\n",
    "        total_params = 0\n",
    "        total_size_bytes = 0\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            total_params += param.numel()\n",
    "            if param.dtype == torch.uint8:\n",
    "                # 4-bit quantized (stored as uint8, but only 4 bits used)\n",
    "                total_size_bytes += param.numel() * 0.5  # 4 bits = 0.5 bytes\n",
    "            else:\n",
    "                total_size_bytes += param.numel() * param.element_size()\n",
    "        \n",
    "        return total_params, total_size_bytes\n",
    "    \n",
    "    orig_params, orig_size = get_model_size(original_model)\n",
    "    quant_params, quant_size = get_model_size(quantized_model)\n",
    "    \n",
    "    # Account for scale and zero_point parameters\n",
    "    for module in quantized_model.modules():\n",
    "        if isinstance(module, QuantizedLinear):\n",
    "            quant_size += 8  # 4 bytes each for scale and zero_point (float32)\n",
    "    \n",
    "    reduction_ratio = orig_size / quant_size\n",
    "    size_reduction_mb = (orig_size - quant_size) / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        'original_size_mb': orig_size / (1024 * 1024),\n",
    "        'quantized_size_mb': quant_size / (1024 * 1024),\n",
    "        'reduction_ratio': reduction_ratio,\n",
    "        'size_reduction_mb': size_reduction_mb,\n",
    "        'original_params': orig_params,\n",
    "        'quantized_params': quant_params\n",
    "    }\n",
    "\n",
    "def quantize_model_4bit(model):\n",
    "    \"\"\"\n",
    "    Quantize a HuggingFace model to 4-bit precision without using BitsAndBytes\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace model to quantize\n",
    "    \n",
    "    Returns:\n",
    "        quantized_model: 4-bit quantized version of the model\n",
    "        quantization_info: Dictionary with quantization statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a deep copy of the model to avoid modifying the original\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_layers = 0\n",
    "    quantized_layers = 0\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    def replace_linear_layers(module, name=\"\"):\n",
    "        \"\"\"Recursively replace Linear layers with QuantizedLinear layers\"\"\"\n",
    "        nonlocal total_layers, quantized_layers\n",
    "        \n",
    "        for child_name, child_module in list(module.named_children()):\n",
    "            full_name = f\"{name}.{child_name}\" if name else child_name\n",
    "            \n",
    "            if isinstance(child_module, nn.Linear):\n",
    "                total_layers += 1\n",
    "                print(f\"Quantizing layer: {full_name}\")\n",
    "                \n",
    "                # Create quantized replacement\n",
    "                quantized_layer = QuantizedLinear(\n",
    "                    in_features=child_module.in_features,\n",
    "                    out_features=child_module.out_features,\n",
    "                    original_weight=child_module.weight.data,\n",
    "                    original_bias=child_module.bias.data if child_module.bias is not None else None\n",
    "                )\n",
    "                \n",
    "                # Replace the layer\n",
    "                setattr(module, child_name, quantized_layer)\n",
    "                quantized_layers += 1\n",
    "                \n",
    "            else:\n",
    "                # Recursively process child modules\n",
    "                replace_linear_layers(child_module, full_name)\n",
    "    \n",
    "    # Start the quantization process\n",
    "    print(\"Starting 4-bit quantization...\")\n",
    "    quantized_model.eval()  # Set to eval mode\n",
    "    \n",
    "    replace_linear_layers(quantized_model)\n",
    "    \n",
    "    # Calculate size reduction\n",
    "    size_info = calculate_model_size_reduction(model, quantized_model)\n",
    "    \n",
    "    # Prepare quantization info\n",
    "    quantization_info = {\n",
    "        'total_layers_found': total_layers,\n",
    "        'layers_quantized': quantized_layers,\n",
    "        'original_parameters': original_params,\n",
    "        'quantization_method': '4-bit asymmetric',\n",
    "        'size_reduction': size_info\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nQuantization completed!\")\n",
    "    print(f\"- Total Linear layers found: {total_layers}\")\n",
    "    print(f\"- Layers quantized: {quantized_layers}\")\n",
    "    print(f\"- Original model size: {size_info['original_size_mb']:.2f} MB\")\n",
    "    print(f\"- Quantized model size: {size_info['quantized_size_mb']:.2f} MB\")\n",
    "    print(f\"- Size reduction: {size_info['reduction_ratio']:.2f}x\")\n",
    "    print(f\"- Space saved: {size_info['size_reduction_mb']:.2f} MB\")\n",
    "    \n",
    "    return quantized_model, quantization_info\n",
    "\n",
    "# Example usage with Llama 3.2 1B\n",
    "def quantize_llama_3_2_1b():\n",
    "    \"\"\"Load and quantize Llama 3.2 1B model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    except ImportError:\n",
    "        print(\"Error: transformers library not found. Install with: pip install transformers\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load Llama 3.2 1B model\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model in float32 (default precision)\n",
    "        original_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=\"cpu\",  # Keep on CPU for quantization\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Model loaded successfully!\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in original_model.parameters()):,}\")\n",
    "        \n",
    "        # Quantize the model\n",
    "        quantized_model, info = quantize_model_4bit(original_model)\n",
    "        \n",
    "        return quantized_model, tokenizer, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Note: You may need to:\")\n",
    "        print(\"1. Accept the license agreement at https://huggingface.co/meta-llama/Llama-3.2-1B\")\n",
    "        print(\"2. Login with: huggingface-cli login\")\n",
    "        return None, None, None\n",
    "\n",
    "def test_llama_quantization():\n",
    "    \"\"\"Test the quantized Llama model with sample text generation\"\"\"\n",
    "    \n",
    "    result = quantize_llama_3_2_1b()\n",
    "    if result[0] is None:\n",
    "        return None\n",
    "    \n",
    "    quantized_model, tokenizer, info = result\n",
    "    \n",
    "    # Test text generation\n",
    "    test_prompt = \"The future of artificial intelligence is\"\n",
    "    print(f\"\\nTesting with prompt: '{test_prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Generate text with quantized model\n",
    "    print(\"\\nGenerating text with quantized model...\")\n",
    "    with torch.no_grad():\n",
    "        quantized_model.eval()\n",
    "        outputs = quantized_model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_length=50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and print results\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated text:\")\n",
    "    print(f\"'{generated_text}'\")\n",
    "    \n",
    "    return quantized_model, tokenizer, info\n",
    "\n",
    "def compare_model_outputs():\n",
    "    \"\"\"Compare outputs between original and quantized models\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    except ImportError:\n",
    "        print(\"Error: transformers library not found.\")\n",
    "        return\n",
    "    \n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    \n",
    "    # Load original model\n",
    "    print(\"Loading original model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Quantize model\n",
    "    print(\"Quantizing model...\")\n",
    "    quantized_model, _ = quantize_model_4bit(original_model)\n",
    "    \n",
    "    # Test prompt\n",
    "    test_prompt = \"Artificial intelligence will\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(f\"\\nComparing outputs for prompt: '{test_prompt}'\")\n",
    "    \n",
    "    # Generate with original model\n",
    "    with torch.no_grad():\n",
    "        original_outputs = original_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=30,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.1,  # Low temperature for more deterministic output\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Generate with quantized model\n",
    "    with torch.no_grad():\n",
    "        quantized_outputs = quantized_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=30,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode outputs\n",
    "    original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "    quantized_text = tokenizer.decode(quantized_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nOriginal model output:\")\n",
    "    print(f\"'{original_text}'\")\n",
    "    print(f\"\\nQuantized model output:\")\n",
    "    print(f\"'{quantized_text}'\")\n",
    "    \n",
    "    # Calculate similarity (simple token overlap)\n",
    "    original_tokens = set(tokenizer.encode(original_text))\n",
    "    quantized_tokens = set(tokenizer.encode(quantized_text))\n",
    "    \n",
    "    if len(original_tokens.union(quantized_tokens)) > 0:\n",
    "        similarity = len(original_tokens.intersection(quantized_tokens)) / len(original_tokens.union(quantized_tokens))\n",
    "        print(f\"\\nToken similarity: {similarity:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Llama 3.2 1B - 4-bit Quantization\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Option 1: Just quantize and get stats\n",
    "    print(\"\\n1. Quantizing Llama 3.2 1B...\")\n",
    "    result = quantize_llama_3_2_1b()\n",
    "    \n",
    "    if result[0] is not None:\n",
    "        print(\"\\n2. Testing text generation...\")\n",
    "        test_llama_quantization()\n",
    "        \n",
    "        print(\"\\n3. Comparing original vs quantized outputs...\")\n",
    "        compare_model_outputs()\n",
    "    else:\n",
    "        print(\"Failed to load model. Please check your Hugging Face access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0273b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.1-8B-Instruct in float16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 8,030,261,248 parameters\n",
      "Starting 4-bit quantization...\n",
      "Quantization complete!\n",
      "├── Layers quantized: 225\n",
      "├── Original size: 14314.0 MB\n",
      "├── Quantized size: 3578.5 MB\n",
      "├── Compression ratio: 4.00x\n",
      "└── Memory saved: 10735.5 MB\n",
      "Ready to use! Load your model like:\n",
      "model, tokenizer = load_and_quantize_model('your-model-name')\n",
      "model = model.to('cuda')  # Move to GPU after quantization\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, weight, bias=None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Convert to float32 for quantization math, then back to storage types\n",
    "        weight_f32 = weight.float()\n",
    "        \n",
    "        # 4-bit quantization with better precision\n",
    "        min_val = weight_f32.min().item()\n",
    "        max_val = weight_f32.max().item()\n",
    "        \n",
    "        # Use full 4-bit range: -8 to 7 (signed) or 0 to 15 (unsigned)\n",
    "        # Let's use signed for better center around zero\n",
    "        qmin, qmax = -8, 7\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = max(qmin, min(qmax, round(zero_point)))\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.clamp(\n",
    "            torch.round(weight_f32 / scale + zero_point), \n",
    "            qmin, qmax\n",
    "        ).to(torch.int8)  # Use int8 for signed 4-bit values\n",
    "        \n",
    "        # Pack two 4-bit values into one byte for TRUE 4-bit storage\n",
    "        flat_q = quantized.flatten()\n",
    "        \n",
    "        # Ensure even length for packing\n",
    "        if len(flat_q) % 2 == 1:\n",
    "            flat_q = torch.cat([flat_q, torch.zeros(1, dtype=torch.int8)])\n",
    "        \n",
    "        # Pack: shift first value left by 4 bits, add second value\n",
    "        # Convert to unsigned for bitwise operations\n",
    "        flat_q_unsigned = flat_q + 8  # Shift signed [-8,7] to unsigned [0,15]\n",
    "        packed = (flat_q_unsigned[::2] << 4) + flat_q_unsigned[1::2]\n",
    "        \n",
    "        # Store as uint8\n",
    "        self.register_buffer('packed_weight', packed.to(torch.uint8))\n",
    "        self.register_buffer('scale', torch.tensor(scale, dtype=torch.float16))\n",
    "        self.register_buffer('zero_point', torch.tensor(zero_point, dtype=torch.int8))\n",
    "        self.register_buffer('original_shape', torch.tensor(weight.shape, dtype=torch.long))\n",
    "        \n",
    "        # Store bias in float16 if exists\n",
    "        if bias is not None:\n",
    "            self.register_buffer('bias', bias.to(torch.float16))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def unpack_weights(self):\n",
    "        \"\"\"Unpack 4-bit weights back to original precision\"\"\"\n",
    "        packed = self.packed_weight\n",
    "        \n",
    "        # Unpack: extract high and low nibbles\n",
    "        high_nibble = (packed >> 4) & 0xF\n",
    "        low_nibble = packed & 0xF\n",
    "        \n",
    "        # Interleave back to original order\n",
    "        unpacked = torch.stack([high_nibble, low_nibble], dim=1).flatten()\n",
    "        \n",
    "        # Convert back to signed [-8, 7] range\n",
    "        unpacked = unpacked.to(torch.int8) - 8\n",
    "        \n",
    "        # Trim to original number of elements\n",
    "        orig_numel = self.original_shape.prod().item()\n",
    "        unpacked = unpacked[:orig_numel]\n",
    "        \n",
    "        # Reshape to original shape\n",
    "        unpacked = unpacked.view(tuple(self.original_shape.tolist()))\n",
    "        \n",
    "        return unpacked\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Unpack and dequantize weights\n",
    "        quantized_weight = self.unpack_weights()\n",
    "        \n",
    "        # Dequantize: convert back to float16 for computation\n",
    "        weight_f16 = (self.scale * (quantized_weight.float() - self.zero_point.float())).half()\n",
    "        \n",
    "        # Ensure input is float16 too\n",
    "        x = x.half()\n",
    "        \n",
    "        return torch.nn.functional.linear(x, weight_f16, self.bias)\n",
    "\n",
    "def quantize_4bit(model):\n",
    "    \"\"\"\n",
    "    Takes a model (preferably loaded in float16) and quantizes it to 4-bit\n",
    "    Returns the same model object with layers replaced\n",
    "    \"\"\"\n",
    "    quantized_layers = 0\n",
    "    total_original_size = 0\n",
    "    total_quantized_size = 0\n",
    "    \n",
    "    def replace_layers(module):\n",
    "        nonlocal quantized_layers, total_original_size, total_quantized_size\n",
    "        \n",
    "        for name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear):\n",
    "                # Calculate original size\n",
    "                original_size = child.weight.numel() * child.weight.element_size()\n",
    "                if child.bias is not None:\n",
    "                    original_size += child.bias.numel() * child.bias.element_size()\n",
    "                total_original_size += original_size\n",
    "                \n",
    "                # Create quantized layer\n",
    "                quantized_layer = QuantizedLinear(\n",
    "                    child.in_features, \n",
    "                    child.out_features, \n",
    "                    child.weight.data, \n",
    "                    child.bias.data if child.bias is not None else None\n",
    "                )\n",
    "                \n",
    "                # Calculate quantized size (approximate)\n",
    "                # Packed weights: ~0.5 bytes per weight + scale + zero_point\n",
    "                quantized_size = (child.weight.numel() * 0.5 +  # 4-bit weights\n",
    "                                2 +  # scale (float16)\n",
    "                                1 +  # zero_point (int8)\n",
    "                                8)   # shape info\n",
    "                if child.bias is not None:\n",
    "                    quantized_size += child.bias.numel() * 2  # bias in float16\n",
    "                total_quantized_size += quantized_size\n",
    "                \n",
    "                # Replace layer\n",
    "                setattr(module, name, quantized_layer)\n",
    "                quantized_layers += 1\n",
    "                \n",
    "                # Clean up\n",
    "                del child\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            else:\n",
    "                replace_layers(child)\n",
    "    \n",
    "    print(\"Starting 4-bit quantization...\")\n",
    "    replace_layers(model)\n",
    "    \n",
    "    # Print statistics\n",
    "    compression_ratio = total_original_size / total_quantized_size\n",
    "    memory_saved_mb = (total_original_size - total_quantized_size) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Quantization complete!\")\n",
    "    print(f\"├── Layers quantized: {quantized_layers}\")\n",
    "    print(f\"├── Original size: {total_original_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"├── Quantized size: {total_quantized_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"├── Compression ratio: {compression_ratio:.2f}x\")\n",
    "    print(f\"└── Memory saved: {memory_saved_mb:.1f} MB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "def load_and_quantize_model(model_name):\n",
    "    \"\"\"\n",
    "    Load model in float16, then quantize to 4-bit\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} in float16...\")\n",
    "    \n",
    "    # Load model in float16 first\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",  # Load on CPU first\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Now quantize to 4-bit\n",
    "    quantized_model = quantize_4bit(model)\n",
    "    \n",
    "    return quantized_model, tokenizer\n",
    "\n",
    "# Test with your model\n",
    "    # Example: Load and quantize Llama 3.2 1B\n",
    "model, tokenizer = load_and_quantize_model(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# After quantization, you can move to GPU:\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "print(\"Ready to use! Load your model like:\")\n",
    "print(\"model, tokenizer = load_and_quantize_model('your-model-name')\")\n",
    "print(\"model = model.to('cuda')  # Move to GPU after quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057d94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.1-8B-Instruct on CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 8,030,261,248 parameters\n",
      "Quantizing on CPU...\n",
      "Starting 4-bit quantization...\n",
      "Quantization complete!\n",
      "├── Layers quantized: 225\n",
      "├── Original size: 14314.0 MB\n",
      "├── Quantized size: 3578.5 MB\n",
      "├── Compression ratio: 4.00x\n",
      "└── Memory saved: 10735.5 MB\n",
      "Moving quantized model to GPU...\n",
      "✅ Model ready for inference with no additional GPU memory overhead!\n",
      "Ready to use! Load your model like:\n",
      "model, tokenizer = load_and_quantize_model('your-model-name')\n",
      "model = model.to('cuda')  # Move to GPU after quantization\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, weight, bias=None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Convert to float32 for quantization math, then back to storage types\n",
    "        weight_f32 = weight.float()\n",
    "        \n",
    "        # 4-bit quantization with better precision\n",
    "        min_val = weight_f32.min().item()\n",
    "        max_val = weight_f32.max().item()\n",
    "        \n",
    "        # Use full 4-bit range: -8 to 7 (signed) or 0 to 15 (unsigned)\n",
    "        # Let's use signed for better center around zero\n",
    "        qmin, qmax = -8, 7\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = max(qmin, min(qmax, round(zero_point)))\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.clamp(\n",
    "            torch.round(weight_f32 / scale + zero_point), \n",
    "            qmin, qmax\n",
    "        ).to(torch.int8)  # Use int8 for signed 4-bit values\n",
    "        \n",
    "        # Pack two 4-bit values into one byte for TRUE 4-bit storage\n",
    "        flat_q = quantized.flatten()\n",
    "        \n",
    "        # Ensure even length for packing\n",
    "        if len(flat_q) % 2 == 1:\n",
    "            flat_q = torch.cat([flat_q, torch.zeros(1, dtype=torch.int8)])\n",
    "        \n",
    "        # Pack: shift first value left by 4 bits, add second value\n",
    "        # Convert to unsigned for bitwise operations\n",
    "        flat_q_unsigned = flat_q + 8  # Shift signed [-8,7] to unsigned [0,15]\n",
    "        packed = (flat_q_unsigned[::2] << 4) + flat_q_unsigned[1::2]\n",
    "        \n",
    "        # Store as uint8\n",
    "        self.register_buffer('packed_weight', packed.to(torch.uint8))\n",
    "        self.register_buffer('scale', torch.tensor(scale, dtype=torch.float16))\n",
    "        self.register_buffer('zero_point', torch.tensor(zero_point, dtype=torch.int8))\n",
    "        self.register_buffer('original_shape', torch.tensor(weight.shape, dtype=torch.long))\n",
    "        \n",
    "        # Store bias in float16 if exists\n",
    "        if bias is not None:\n",
    "            self.register_buffer('bias', bias.to(torch.float16))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def unpack_weights(self):\n",
    "        \"\"\"Unpack 4-bit weights back to original precision\"\"\"\n",
    "        packed = self.packed_weight\n",
    "        \n",
    "        # Unpack: extract high and low nibbles\n",
    "        high_nibble = (packed >> 4) & 0xF\n",
    "        low_nibble = packed & 0xF\n",
    "        \n",
    "        # Interleave back to original order\n",
    "        unpacked = torch.stack([high_nibble, low_nibble], dim=1).flatten()\n",
    "        \n",
    "        # Convert back to signed [-8, 7] range\n",
    "        unpacked = unpacked.to(torch.int8) - 8\n",
    "        \n",
    "        # Trim to original number of elements\n",
    "        orig_numel = self.original_shape.prod().item()\n",
    "        unpacked = unpacked[:orig_numel]\n",
    "        \n",
    "        # Reshape to original shape\n",
    "        unpacked = unpacked.view(tuple(self.original_shape.tolist()))\n",
    "        \n",
    "        return unpacked\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with TRUE 4-bit computation - no dequantization to float16!\n",
    "        Uses 4-bit arithmetic similar to BnB\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        \n",
    "        # Keep everything in quantized form - no full dequantization\n",
    "        # Move quantized tensors to device\n",
    "        packed_weight = self.packed_weight.to(device, non_blocking=True)\n",
    "        scale = self.scale.to(device, non_blocking=True)\n",
    "        zero_point = self.zero_point.to(device, non_blocking=True)\n",
    "        \n",
    "        # Ensure input is float16\n",
    "        if x.dtype != torch.float16:\n",
    "            x = x.half()\n",
    "        \n",
    "        # Compute output using 4-bit quantized matrix multiplication\n",
    "        # This is the key: we avoid creating full float16 weight matrix\n",
    "        output = self._quantized_matmul(x, packed_weight, scale, zero_point)\n",
    "        \n",
    "        # Add bias if exists\n",
    "        if self.bias is not None:\n",
    "            bias = self.bias.to(device, non_blocking=True)\n",
    "            output = output + bias\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def _quantized_matmul(self, x, packed_weight, scale, zero_point):\n",
    "        \"\"\"\n",
    "        Perform matrix multiplication in 4-bit space without full dequantization\n",
    "        This mimics BnB's approach of computing with quantized values\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        out_features = self.out_features\n",
    "        \n",
    "        # Unpack weights in small chunks to save memory\n",
    "        chunk_size = 1024  # Process in chunks\n",
    "        output = torch.zeros(batch_size, seq_len, out_features, dtype=x.dtype, device=x.device)\n",
    "        \n",
    "        for i in range(0, out_features, chunk_size):\n",
    "            end_idx = min(i + chunk_size, out_features)\n",
    "            chunk_rows = end_idx - i\n",
    "            \n",
    "            # Calculate which packed elements we need for this chunk\n",
    "            elements_per_row = in_features\n",
    "            start_packed = i * elements_per_row // 2\n",
    "            end_packed = end_idx * elements_per_row // 2\n",
    "            \n",
    "            if end_packed > len(packed_weight):\n",
    "                end_packed = len(packed_weight)\n",
    "            \n",
    "            # Unpack only this chunk\n",
    "            chunk_packed = packed_weight[start_packed:end_packed]\n",
    "            chunk_unpacked = self._unpack_chunk(chunk_packed, chunk_rows, in_features)\n",
    "            \n",
    "            # Dequantize only this small chunk\n",
    "            chunk_weights = scale * (chunk_unpacked.half() - zero_point.half())\n",
    "            \n",
    "            # Compute chunk of output\n",
    "            output[:, :, i:end_idx] = torch.matmul(x, chunk_weights.T)\n",
    "            \n",
    "            # Clean up chunk immediately\n",
    "            del chunk_unpacked, chunk_weights\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def _unpack_chunk(self, packed_chunk, rows, cols):\n",
    "        \"\"\"Unpack a small chunk of weights\"\"\"\n",
    "        # Unpack nibbles\n",
    "        high_nibble = (packed_chunk >> 4) & 0xF\n",
    "        low_nibble = packed_chunk & 0xF\n",
    "        \n",
    "        # Interleave\n",
    "        unpacked = torch.stack([high_nibble, low_nibble], dim=1).flatten()\n",
    "        \n",
    "        # Convert to signed and reshape\n",
    "        unpacked = unpacked.to(torch.int8) - 8\n",
    "        \n",
    "        # Trim and reshape to chunk size\n",
    "        needed_elements = rows * cols\n",
    "        unpacked = unpacked[:needed_elements]\n",
    "        unpacked = unpacked.view(rows, cols)\n",
    "        \n",
    "        return unpacked\n",
    "\n",
    "def quantize_4bit(model):\n",
    "    \"\"\"\n",
    "    Takes a model (preferably loaded in float16) and quantizes it to 4-bit\n",
    "    Returns the same model object with layers replaced\n",
    "    \"\"\"\n",
    "    quantized_layers = 0\n",
    "    total_original_size = 0\n",
    "    total_quantized_size = 0\n",
    "    \n",
    "    def replace_layers(module):\n",
    "        nonlocal quantized_layers, total_original_size, total_quantized_size\n",
    "        \n",
    "        for name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear):\n",
    "                # Calculate original size\n",
    "                original_size = child.weight.numel() * child.weight.element_size()\n",
    "                if child.bias is not None:\n",
    "                    original_size += child.bias.numel() * child.bias.element_size()\n",
    "                total_original_size += original_size\n",
    "                \n",
    "                # Create quantized layer\n",
    "                quantized_layer = QuantizedLinear(\n",
    "                    child.in_features, \n",
    "                    child.out_features, \n",
    "                    child.weight.data, \n",
    "                    child.bias.data if child.bias is not None else None\n",
    "                )\n",
    "                \n",
    "                # Calculate quantized size (approximate)\n",
    "                # Packed weights: ~0.5 bytes per weight + scale + zero_point\n",
    "                quantized_size = (child.weight.numel() * 0.5 +  # 4-bit weights\n",
    "                                2 +  # scale (float16)\n",
    "                                1 +  # zero_point (int8)\n",
    "                                8)   # shape info\n",
    "                if child.bias is not None:\n",
    "                    quantized_size += child.bias.numel() * 2  # bias in float16\n",
    "                total_quantized_size += quantized_size\n",
    "                \n",
    "                # Replace layer\n",
    "                setattr(module, name, quantized_layer)\n",
    "                quantized_layers += 1\n",
    "                \n",
    "                # Clean up\n",
    "                del child\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            else:\n",
    "                replace_layers(child)\n",
    "    \n",
    "    print(\"Starting 4-bit quantization...\")\n",
    "    replace_layers(model)\n",
    "    \n",
    "    # Print statistics\n",
    "    compression_ratio = total_original_size / total_quantized_size\n",
    "    memory_saved_mb = (total_original_size - total_quantized_size) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Quantization complete!\")\n",
    "    print(f\"├── Layers quantized: {quantized_layers}\")\n",
    "    print(f\"├── Original size: {total_original_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"├── Quantized size: {total_quantized_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"├── Compression ratio: {compression_ratio:.2f}x\")\n",
    "    print(f\"└── Memory saved: {memory_saved_mb:.1f} MB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_quantize_and_move_to_gpu(model_name):\n",
    "    \"\"\"\n",
    "    Load on CPU -> Quantize on CPU -> Move to GPU (BnB style)\n",
    "    No additional memory consumption during inference\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} on CPU...\")\n",
    "    \n",
    "    # 1. Load model on CPU in float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # 2. Quantize on CPU (no memory spike)\n",
    "    print(\"Quantizing on CPU...\")\n",
    "    quantized_model = quantize_4bit(model)\n",
    "    \n",
    "    # 3. Move quantized model to GPU (only 4-bit weights transferred)\n",
    "    print(\"Moving quantized model to GPU...\")\n",
    "    quantized_model = quantized_model.to(\"cuda\")\n",
    "    \n",
    "    # 4. Clear CPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"✅ Model ready for inference with no additional GPU memory overhead!\")\n",
    "    \n",
    "    return quantized_model, tokenizer\n",
    "\n",
    "# Memory-efficient inference function\n",
    "def efficient_inference_test(model_name=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Complete pipeline: CPU quantize -> GPU move -> Inference (BnB style)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load, quantize on CPU, move to GPU\n",
    "    model, tokenizer = load_quantize_and_move_to_gpu(model_name)\n",
    "    \n",
    "    print(f\"\\n💾 GPU Memory after loading:\")\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\n💬 Starting efficient inference (no memory growth)...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n👤 You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"👋 Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Check GPU memory before inference\n",
    "        if torch.cuda.is_available():\n",
    "            mem_before = torch.cuda.memory_allocated() / 1024**3\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(user_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with no memory growth\n",
    "        print(\"🤖 Generating...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # Check GPU memory after inference\n",
    "        if torch.cuda.is_available():\n",
    "            mem_after = torch.cuda.memory_allocated() / 1024**3\n",
    "            mem_growth = mem_after - mem_before\n",
    "        \n",
    "        # Decode response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_response = full_response[len(user_input):].strip()\n",
    "        \n",
    "        print(f\"🤖 Assistant: {assistant_response}\")\n",
    "        print(f\"   ⏱️  {generation_time:.1f}s\", end=\"\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\" | 📊 Memory growth: {mem_growth:.3f} GB\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        # Minimal cleanup (should be near zero growth)\n",
    "        del inputs, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Test with your model\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Load and quantize Llama 3.2 1B\n",
    "    model, tokenizer = load_quantize_and_move_to_gpu(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "    \n",
    "    # After quantization, you can move to GPU:\n",
    "    model = model.to(\"cuda\")\n",
    "    \n",
    "    print(\"Ready to use! Load your model like:\")\n",
    "    print(\"model, tokenizer = load_and_quantize_model('your-model-name')\")\n",
    "    print(\"model = model.to('cuda')  # Move to GPU after quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecc5604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   6151]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "187.9736738204956\n"
     ]
    }
   ],
   "source": [
    "message='hi'\n",
    "to=tokenizer(message,return_tensors='pt').to('cuda')\n",
    "print(to)\n",
    "start=time.time()\n",
    "out=model.generate(**to,max_new_tokens=100)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69462837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>hiæk�adieravaluczuczuczучаreatreatuczucz rencontgan addObserveruczuczuczuczucz Percgan Boehucz Bearduczuczuczgan ReyesuczτανuczohnuczuczuczuczuczuczuczuczuczuczuczuczuczuczuczuczuczuczriluczgandejuczWISE般_SDWISEWISE Lauderdale LauderdaleWISE竹竹rilucz竹Tokenizeruczril Lauderdale竹 Percrilucz LauderdaleTokenizeruczhay Perciten竹TokenizerrilWISEWISE竹rilTokenizerucz竹254 Lauderdale ReyesWISEucz竹\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75450f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
