{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75983d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå pipeline_tag: text-generation\n",
      "üìå auto_map: None\n",
      "üìå Uses chat template: ‚úÖ\n",
      "‚ö†Ô∏è Might require specific model class or loading logic\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Get HF model info\n",
    "info = model_info(model_id)\n",
    "print(\"üìå pipeline_tag:\", info.pipeline_tag)\n",
    "\n",
    "# Load model config\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "auto_map = getattr(config, \"auto_map\", None)\n",
    "print(\"üìå auto_map:\", auto_map)\n",
    "\n",
    "# Load tokenizer to check chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "chat_template = getattr(tokenizer, \"chat_template\", None)\n",
    "print(\"üìå Uses chat template:\", \"‚úÖ\" if chat_template else \"‚ùå\")\n",
    "\n",
    "# Final decision: is it usable with AutoModelForCausalLM?\n",
    "if auto_map and \"AutoModelForCausalLM\" in auto_map:\n",
    "    print(\"‚úÖ Can use with AutoModelForCausalLM (trust_remote_code=True)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Might require specific model class or loading logic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe38f6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA T1000 8GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f841bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:29<00:00,  7.45s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model= AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    max_memory={\n",
    "        0:'5GB',\n",
    "        1:\"5GB\",\n",
    "        'cpu':\"16GB\"\n",
    "    },\n",
    "    offload_folder=\"/offload_nvm\",\n",
    "    offload_state_dict=True,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352348e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ce3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0052, -0.0293, -0.0064,  ...,  0.0092, -0.0415, -0.0269],\n",
      "        [-0.0150, -0.0679, -0.0059,  ..., -0.0149, -0.0498,  0.0197],\n",
      "        [-0.0173, -0.0391, -0.0040,  ...,  0.0107, -0.0132,  0.0071],\n",
      "        ...,\n",
      "        [-0.0035, -0.0383,  0.0781,  ...,  0.0057, -0.0012,  0.0024],\n",
      "        [-0.0033, -0.0093,  0.0437,  ...,  0.0047, -0.0011,  0.0012],\n",
      "        [-0.0019, -0.0153,  0.0347,  ...,  0.0111,  0.0004,  0.0042]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[-0.0352, -0.0178, -0.0166,  ..., -0.0297,  0.0003, -0.0120],\n",
      "        [-0.0032,  0.0012, -0.0082,  ..., -0.0229,  0.0017,  0.0066],\n",
      "        [ 0.0002,  0.0092, -0.0227,  ..., -0.0153, -0.0040, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0557,  0.0065, -0.0040,  ...,  0.0128,  0.0164, -0.0405],\n",
      "        [-0.0173, -0.0264, -0.0415,  ...,  0.0391,  0.0240,  0.0092],\n",
      "        [ 0.0238,  0.0258, -0.0001,  ..., -0.0137, -0.0101, -0.0140]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[-0.0291,  0.0149,  0.0017,  ...,  0.0090,  0.0167, -0.0088],\n",
      "        [-0.0166, -0.0232, -0.0205,  ..., -0.0117,  0.0033, -0.0047],\n",
      "        [-0.0052,  0.0015, -0.0020,  ...,  0.0295, -0.0118,  0.0063],\n",
      "        ...,\n",
      "        [-0.0043,  0.0151,  0.0093,  ..., -0.0033,  0.0044, -0.0056],\n",
      "        [-0.0096, -0.0176, -0.0079,  ...,  0.0007, -0.0209,  0.0069],\n",
      "        [ 0.0261,  0.0045,  0.0062,  ..., -0.0117,  0.0273, -0.0072]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[ 0.0065,  0.0096,  0.0047,  ...,  0.0166,  0.0109,  0.0090],\n",
      "        [ 0.0127,  0.0193,  0.0117,  ...,  0.0021,  0.0167,  0.0078],\n",
      "        [ 0.0154,  0.0080, -0.0193,  ...,  0.0089,  0.0060,  0.0049],\n",
      "        ...,\n",
      "        [-0.0129,  0.0293,  0.0037,  ...,  0.0090, -0.0069,  0.0058],\n",
      "        [-0.0518,  0.0193, -0.0098,  ..., -0.0159,  0.0078, -0.0009],\n",
      "        [-0.0173, -0.0051, -0.0204,  ..., -0.0109,  0.0028,  0.0255]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[-0.0030, -0.0110, -0.0070,  ...,  0.0004,  0.0200, -0.0270],\n",
      "        [-0.0295, -0.0121, -0.0067,  ..., -0.0165,  0.0101, -0.0074],\n",
      "        [-0.0088, -0.0047,  0.0244,  ...,  0.0117, -0.0115,  0.0045],\n",
      "        ...,\n",
      "        [-0.0198,  0.0294, -0.0073,  ...,  0.0011, -0.0261, -0.0099],\n",
      "        [ 0.0051,  0.0005,  0.0425,  ..., -0.0400,  0.0198, -0.0070],\n",
      "        [-0.0217, -0.0015, -0.0007,  ...,  0.0247, -0.0242,  0.0212]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# next(model[0].parameters()).device\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(next(i.parameters()).to(\"cuda:0\"))\n",
    "    # break\n",
    "# next(model[0].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d54bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     layer\u001b[38;5;241m.\u001b[39mto(device0)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1348\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    print(\"FlashAttention library imported successfully.\")\n",
    "    # Try a minimal dummy call (ensure a CUDA device is available)\n",
    "    if torch.cuda.is_available():\n",
    "        q = torch.randn(1, 1024, 12, 64, dtype=torch.float16, device='cuda')\n",
    "        k = torch.randn(1, 1024, 12, 64, dtype=torch.float16, device='cuda')\n",
    "        v = torch.randn(1, 1024, 12, 64, dtype=torch.float16, device='cuda')\n",
    "        _ = flash_attn_func(q, k, v)\n",
    "        print(\"FlashAttention dummy run successful.\")\n",
    "    else:\n",
    "        print(\"CUDA not available for dummy FlashAttention test.\")\n",
    "except ImportError:\n",
    "    print(\"FlashAttention library NOT imported or functional.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during FlashAttention dummy run: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d37d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
