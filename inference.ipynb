{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3c95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cuda')\n",
    "question=\"hi how are you\"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "out=model.generate(**prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc7fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6151,   1268,    527,    499,     30,    602,    574,  20910,\n",
       "           422,    499,   1436,   1520,    757,    449,   2555,     11,    602,\n",
       "           617,    264,    220,   2550,     22,    220,     19],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef9e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi how are you? i was wondering if you could help me with something, i have a 1997 4\n"
     ]
    }
   ],
   "source": [
    "# If out is a tensor (e.g., shape [1, seq_len])\n",
    "decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08669892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47aff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b28405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ea19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa53c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=input()\n",
    "converstation=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are an assitant help user by solving their query\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"query-{question}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe16aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "# prompt.to('cpu'//)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95944161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   6151,   1268,    527,    220]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d89c9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out=model.generate(**prompt,max_new_tokens=1000)\n",
    "# print(tok.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "059c40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=prompt['input_ids']\n",
    "del prompt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2480b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "# del input_ids\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.decoder.embed_positions.to('cuda')\n",
    "# x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "# model.model.decoder.embed_positions.to('cpu')\n",
    "# # del model.model.decoder.embed_positions\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "976c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max_position_embeddings: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's max_position_embeddings: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b082e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b42bffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "cnt=0\n",
    "for i in model.model.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    i.to('cuda')\n",
    "    x.to('cuda')\n",
    "    # print(x)\n",
    "    with torch.no_grad():\n",
    "        x=i(x,position_embeddings=(cos,sin))\n",
    "        # print(x)\n",
    "    i.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    x=x[0]\n",
    "    # x.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "        # time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18251d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f129a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7762, -5.0166, 13.8534,  ..., -1.9152,  1.8989, -3.0755],\n",
       "         [-0.2314,  1.3026,  1.7797,  ..., -0.2708, -1.0364, -0.2083],\n",
       "         [ 0.2276,  0.8936,  0.8521,  ..., -0.0246, -1.4944, -0.0458],\n",
       "         [ 0.0748,  1.2204,  0.5945,  ..., -0.7305, -1.4607, -0.6370],\n",
       "         [ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc202f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef1caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.model.norm.to('cuda')\n",
    "    x=model.model.norm(x)\n",
    "    model.model.norm.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cuda')\n",
    "    x=model.lm_head(x)\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21f836c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4aca9e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "560056fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bc43b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x1=x[:,-1,:]\n",
    "x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "# print(tok.decode(x1[0]),end=\" \")\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a20b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "        input_ids=prompt['input_ids'].to('cuda')\n",
    "        attention_mask=prompt['attention_mask']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        input_ids.to('cpu')\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # Safe check\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        cnt=0\n",
    "        for i in model.model.layers:\n",
    "            cnt+=1\n",
    "            # print(cnt)\n",
    "            i.to('cuda')\n",
    "            x.to('cuda')\n",
    "            # print(x)\n",
    "            with torch.no_grad():\n",
    "                x=i(x,position_embeddings=(cos,sin))\n",
    "                # print(x)\n",
    "            i.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x=x[0]\n",
    "            # x.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9202f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt_text, tokens=250):\n",
    "    # Initial tokenization\n",
    "    prompt = tok(prompt_text, return_tensors='pt')\n",
    "    input_ids = prompt['input_ids'].to('cuda')  # [1, seq_len]\n",
    "    del prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # Run only the latest token through embed layer and model\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for layer in model.model.layers:\n",
    "            layer.to('cuda')\n",
    "            x = x.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                x = layer(x, position_embeddings=(cos, sin))\n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x = x[0]  # Remove tuple\n",
    "\n",
    "        # Final norm and lm_head\n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]  # Only the last token matters\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # shape [1,1]\n",
    "\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode and print latest token\n",
    "        print(tok.decode(next_token[0]), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506a694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am very happy to meet you all. I am a very friendly person and I love to meet new people. I am a very good listener and I am very easy going. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi how are you ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(prompt_text, tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Only the last token matters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi how are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39d98000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_oov(text, tokenizer, max_token_id=2048, oov_token='[OOV]'):\n",
    "    # First, add [OOV] token to tokenizer if not present\n",
    "    if oov_token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [oov_token]})\n",
    "\n",
    "    oov_id = tokenizer.convert_tokens_to_ids(oov_token)\n",
    "\n",
    "    # Raw tokenization\n",
    "    encoding = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "\n",
    "    # Replace tokens ≥ max_token_id with OOV\n",
    "    processed_ids = []\n",
    "    for token_id in input_ids:\n",
    "        if token_id >= max_token_id:\n",
    "            processed_ids.append(oov_id)\n",
    "        else:\n",
    "            processed_ids.append(token_id)\n",
    "\n",
    "    return torch.tensor([processed_ids]), encoding['attention_mask']\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    try:\n",
    "        for _ in range(tokens):\n",
    "            # prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "            # input_ids = prompt['input_ids']\n",
    "            # attention_mask = prompt['attention_mask']\n",
    "            input_ids, attention_mask = tokenize_with_oov(question, tok)\n",
    "\n",
    "            del prompt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Truncate if too long\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            max_positions = model.config.max_position_embeddings\n",
    "            if seq_len > max_positions:\n",
    "                input_ids = input_ids[:, -max_positions:]\n",
    "                attention_mask = attention_mask[:, -max_positions:]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "            # Embeddings\n",
    "            model.model.decoder.embed_tokens.to('cpu')\n",
    "            x = model.model.decoder.embed_tokens(input_ids)\n",
    "\n",
    "            # Positional IDs\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device='cpu')\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            model.model.decoder.embed_positions.to('cpu')\n",
    "            x = x + model.model.decoder.embed_positions(position_ids)\n",
    "\n",
    "            # Decoder layers\n",
    "            for layer in model.model.decoder.layers:\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                with torch.no_grad():\n",
    "                    x = layer(x, attention_mask=attention_mask.to(torch.float32))[0]\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Final LM head\n",
    "            model.lm_head.to('cpu')\n",
    "            logits = model.lm_head(x.to('cpu'))\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Decode\n",
    "            next_token_id = logits.argmax(-1)\n",
    "            next_token_text = tok.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            print(next_token_text, end=\" \")\n",
    "            question += \" \" + next_token_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Exception Caught]\")\n",
    "        print(\"Question So Far:\", question)\n",
    "        prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "        input_ids = prompt['input_ids']\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        y = torch.arange(x.shape[1]).unsqueeze(0)\n",
    "        print(\"Embedding Shape:\", x[0].shape)\n",
    "        print(\"Position ID Shape:\", y.shape)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbc1b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy how are you 're post .''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy\n",
      "\n",
      "torch.Size([35, 768])\n",
      "torch.Size([1, 35])\n",
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "model_inference(\"how are you 're post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Ensure model components are initially on CPU\n",
    "    model.model.decoder.embed_tokens.to('cpu')\n",
    "    model.model.decoder.embed_positions.to('cpu')\n",
    "    for layer in model.model.decoder.layers:\n",
    "        layer.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    \n",
    "    max_position_embeddings = model.config.max_position_embeddings\n",
    "    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n",
    "\n",
    "    # Initialize input_ids_for_next_step. This will be the tensor that grows.\n",
    "    input_ids_for_next_step = tok(question, return_tensors='pt')['input_ids'] \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # The 'x' variable for the current iteration is the input_ids_for_next_step\n",
    "        current_input_ids = input_ids_for_next_step \n",
    "        current_seq_len = current_input_ids.shape[1]\n",
    "\n",
    "        # --- Truncation ---\n",
    "        if current_seq_len > max_position_embeddings:\n",
    "            current_input_ids = current_input_ids[:, current_seq_len - max_position_embeddings:]\n",
    "            print(f\"Warning: Sequence truncated from {current_seq_len} to {current_input_ids.shape[1]} for max_position_embeddings.\")\n",
    "            current_seq_len = current_input_ids.shape[1] # Update after truncation\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # Use current_input_ids for embedding lookup\n",
    "        x_embeddings = model.model.decoder.embed_tokens(current_input_ids.to('cpu')) # x_embeddings is on CPU\n",
    "        model.model.decoder.embed_tokens.to('cpu') # Move layer back\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Add Positional Embeddings\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        try:\n",
    "            position_offset = model.model.decoder.embed_positions.offset\n",
    "        except AttributeError:\n",
    "            position_offset = 2 # Common default for OPT\n",
    "\n",
    "        pos_ids = torch.arange(position_offset, position_offset + current_seq_len, device='cpu').unsqueeze(0) \n",
    "        \n",
    "        # The main `x` variable throughout the layer processing will be `current_token_embeddings_plus_pos`\n",
    "        current_token_embeddings_plus_pos = x_embeddings + model.model.decoder.embed_positions(pos_ids) \n",
    "        model.model.decoder.embed_positions.to('cpu') \n",
    "        del x_embeddings \n",
    "        del pos_ids \n",
    "        gc.collect()\n",
    "\n",
    "        # --- Decoder Layers Loop ---\n",
    "        # Rename 'x' to something that reflects its current state (embeddings in, embeddings out)\n",
    "        x_current_layer_output = current_token_embeddings_plus_pos # Start of layer processing\n",
    "        for j, layer in enumerate(model.model.decoder.layers):\n",
    "            try:\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x_current_layer_output = layer(x_current_layer_output)[0]\n",
    "\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "                \n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder layer {j}: {e}\")\n",
    "                break\n",
    "        else: \n",
    "            # --- LM Head and Token Generation ---\n",
    "            model.lm_head.to('cpu')\n",
    "            # The input to lm_head is x_current_layer_output (the final embeddings from decoder)\n",
    "            logits = model.lm_head(x_current_layer_output.to('cpu')) \n",
    "            model.lm_head.to('cpu') \n",
    "            \n",
    "            logits = logits.to('cpu') # Ensure logits are on CPU\n",
    "            gc.collect()\n",
    "\n",
    "            # Get the predicted token ID (on CPU)\n",
    "            # Take the last token's prediction from batch 0\n",
    "            predicted_token_id = logits.argmax(-1)[:, -1].item() \n",
    "\n",
    "            decoded_token = tok.decode(predicted_token_id)\n",
    "            print(decoded_token, end=\" \")\n",
    "            \n",
    "            # --- Prepare input_ids_for_next_step for the next iteration ---\n",
    "            # Append the new token ID to the input_ids_for_next_step tensor (on CPU)\n",
    "            new_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # This is the correct concatenation for input_ids\n",
    "            input_ids_for_next_step = torch.cat((input_ids_for_next_step, new_token_tensor), dim=1)\n",
    "            \n",
    "            # Update the 'question' string for printing and future reference\n",
    "            question = question + \" \" + decoded_token\n",
    "            \n",
    "        if 'predicted_token_id' not in locals(): # If inner loop broke, stop outer loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e86df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "        input_ids=prompt['input_ids']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cuda')\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # del model.model.decoder.embed_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_positions.to('cuda')\n",
    "        x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        # del model.model.decoder.embed_positions\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in model.model.decoder.layers:\n",
    "            try:\n",
    "                i.to('cuda')\n",
    "                x=x.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    x=i(x)[0]\n",
    "                i.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                x=x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                # time.sleep(3)\n",
    "            except:\n",
    "                print(x)\n",
    "                break\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x.to('cuda'))\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\" \")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. your retarded're .''. you retarded're .dylib mom retard.''. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:76\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, past_key_values_length:]\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m]),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     48\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtok(question,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mprompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "model_inference(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "cnt=0\n",
    "for i in model.model.decoder.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    try:\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            x=i(x)[0]\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(3)\n",
    "    except:\n",
    "        print(cnt)\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33d7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.to('cuda')\n",
    "x=model.lm_head(x.to('cuda'))\n",
    "model.lm_head.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e12f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7d35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7274f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
